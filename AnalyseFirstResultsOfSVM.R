#remove old objects for safety resons
rm(list=ls(all=TRUE))

#utility function
glue<-function(...){paste(...,sep="")}

#TODO Adapt to working dir or remove!
#setwd("D:/Documents/MIRI/Semestre 2/APRENDIZAJE AUTOMATICO BASADO EN KERNEL Y MODELADO MULTIVARIANTE/ProyectoFinal/KernelFinalProject/code")
#setwd("E:/Documents/Mis Documentos/MIRI/Semestre 2/APRENDIZAJE AUTOMATICO BASADO EN KERNEL Y MODELADO MULTIVARIANTE/ProyectoFinal/KernelFinalProject/code")
#setwd("J:/UPC/2016/02/KMLMM/KernelMethods/practicals/term_project/code")

#define path of standard directories
source("workingDir.R")
setwd(dataDir)
d<-read.csv("Results_Simulation_SVM_KMLMM_term_project_2016_12_09.csv",sep=";") #<-set to static
str(d)
#there was a mistake in the first run: num.observations and num.vars were mixed up! why?
names(d)[3]<-"num.observations"
names(d)[4]<-"num.vars"

with(d,plot(signal.to.noise.ratio, cv.mean)) #P.P. the plot is already here?
#not very informative, (P.P. everyone) nearly always 1!
with(d,hist(cv.mean))
#not very informative, nearly always 1!

#Why is the explained variance so freaking high?
#Which attributes explain low model quality?
#Too few variables? #P.P. I suppose this is the correction you talked me about

d_low_model_quality<-subset(d,cv.mean<0.9)

with(d, hist(num.vars))
with(d, hist(num.observations))
#Conclusion: the only data sets which were not perfectly well explained were those with the highest number of variables
#which were 13 and 11 and a relatively low number of observations!
#P.P. is this correct? I mean Tomas mentioned that a very good model is not probably correct, I know we know it is correct but
#this is just because we generate the data, ask

with(d_low_model_quality, table(print(num.observations/num.vars)) )
#5.23076923076923                8 
#10               10 

with(d, hist(num.observations/num.vars) )#todo: all this plots in a fancy way->r graphics cookbook
#a ratio of 5 and 8 is at the lower end of the distribution!!!
#Conclusion: We have to assure a ratio of observations/variables lower than 10 to get interesting results!

with(d, plot( num.observations/num.vars, cv.mean))

#Are there any other relevant attributes?
with(d, plot( signal.to.noise.ratio, cv.mean)) #most of them are one
with(d, plot( polynomial.degree, cv.mean))
#The imperfect model only occurred with data generated by polynomial degree 3!#P.P. why?

#*****************************************************************************************
#Conclusion:
#In order to get more interesting results we have to adapt the latin hypercube:
#1) the ratio between num of observations to num of variable should be lower than 10
#2) at least 10 features
#3) polynomial degree three and higher
#4) signal-to-noise ratio higher than 0.3
#*****************************************************************************************

#P.P. copy from tests done in isolated environment# values to consider: cv.mean, sparcity, sd.sparsity, 
#signal to noise ratio, comput time
#
#install.packages("ggplot2")
library(ggplot2)
#signal to noise ratio vs cv.mean
plot(d_low_model_quality$signal.to.noise.ratio, d_low_model_quality$cv.mean)
ggplot(d,aes(x=d$cv.mean,y=d$signal.to.noise.ratio,fill=d$id_parameter_combination))+geom_bar(position = "dodge",stat="identity")+labs(title="Comparison for CV Mean and STNR",x="CV Mean",y="Signal to noise ratio")
ggplot(d,aes(x=d$cv.mean,y=d$sparsity,fill=d$id_parameter_combination))+geom_bar(position = "identity",stat="identity")+labs(title="Comparison for CV Mean and STNR",x="CV Mean",y="Sparcity")
ggplot(d,aes(x=d$sparsity,y=d$signal.to.noise.ratio,fill=d$id_parameter_combination))+geom_bar(position = "identity",stat="identity")+labs(title="Comparison for CV Mean and STNR",x="Sparcity",y="Signal to noise ratio") 

#shows the same concentration, most of them tend to be more grouped in 1 (100 in the graph)
#ggplot(d_low_model_quality,aes(x=cv.mean,y=signal.to.noise.ratio,fill=id_parameter_combination))+geom_bar(position = "dodge",stat="identity")+labs(title="Comparison for CV Mean and STNR",x="CV Mean",y="Signal to noise ratio") 

#get the computation time for each parameter, in total 100 combinations,3 each
hist(tapply(d$compu.time,d$id_parameter_combination,sum))

#define more relevant graphic i.e. most and best descriptive graphic
#pag. 28 <- reorder bars
#pag. 122 <- multiple histograms with different color
#pag. 159 <- error bars
#boxplots? ts?

#todo: is relevant to know the relation between str, cv.mean and sparcity?