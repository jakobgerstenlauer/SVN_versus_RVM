#remove old objects for safety resons
rm(list=ls(all=TRUE))

#utility function
glue<-function(...){paste(...,sep="")}

#TODO Adapt to working dir or remove!
#setwd("D:/Documents/MIRI/Semestre 2/APRENDIZAJE AUTOMATICO BASADO EN KERNEL Y MODELADO MULTIVARIANTE/ProyectoFinal/KernelFinalProject/code")
setwd("E:/Documents/Mis Documentos/MIRI/Semestre 2/APRENDIZAJE AUTOMATICO BASADO EN KERNEL Y MODELADO MULTIVARIANTE/ProyectoFinal/KernelFinalProject/code")
#setwd("J:/UPC/2016/02/KMLMM/KernelMethods/practicals/term_project/code")

#define path of standard directories
source("workingDir.R")

setwd(dataDir)
d<-read.csv("Results_Simulation_SVM_KMLMM_term_project_2016_12_16.csv",sep=";") #<-set to static

#opens a dialog box to input the file
#data.file <- file.choose()
#d  <- read.csv(data.file,sep=";")

setwd(codeDir)
source("KMLMM_term_project_GERSTENLAUER_utility_functions.R")
str(d)

with(subset(d,cv.mean>-0.1),plot(signal.to.noise.ratio, cv.mean)) 
#we can infer that a lot of data tend to 0, i.e. explains 0% of results.
#Also, the majority of the points are pretty fairly distributed in the space, 
#so there is not a strong correlation between signal to noise ratio 
#and the mean cross-validated R2 
with(subset(d,cv.mean>-0.1),hist(cv.mean))

d_low_model_quality<-subset(d,cv.mean<0.9)

with(subset(d,cv.mean>-0.1), hist(num.vars))
with(subset(d,cv.mean>-0.1), hist(num.observations))
#Conclusion: the only data sets which were not perfectly well explained were those with the highest number of variables
#which were 13 and 11 and a relatively low number of observations!
#P.P. is this correct? I mean Tomas mentioned that a very good model is not probably correct, I know we know it is correct but
#this is just because we generate the data, ask

with(d_low_model_quality, table(print(num.observations/num.vars)) )
#5.23076923076923                8 
#10               10 

with(d, hist(num.observations/num.vars) )#todo: all this plots in a fancy way->r graphics cookbook
#a ratio of 5 and 8 is at the lower end of the distribution!!!
#Conclusion: We have to assure a ratio of observations/variables lower than 10 to get interesting results!

with(d, plot( num.observations/num.vars, cv.mean))

#Are there any other relevant attributes?
with(d, plot( signal.to.noise.ratio, cv.mean)) #most of them are one
with(d, plot( polynomial.degree, cv.mean))
#The imperfect model only occurred with data generated by polynomial degree 3!#P.P. why?

#*****************************************************************************************
#Conclusion:
#In order to get more interesting results we have to adapt the latin hypercube:
#1) the ratio between num of observations to num of variable should be lower than 10
#2) at least 10 features
#3) polynomial degree three and higher
#4) signal-to-noise ratio higher than 0.3
#*****************************************************************************************

#get the table analysing the results, in here we use a pearson covariance
row.attributes<-c("d$c_setting","d$epsilon_setting","d$polynomial.degree-d$polynomial_degree_setting","d$sparsity","d$cv.mean","d$compu.time")
column.attributes<-c("d$signal.to.noise.ratio","d$num.observations/d$num.vars","d$polynomial.degree")
result.table.svm <- populate.table(row.attributes,column.attributes,"spearman")
colnames(result.table.svm) <- c("Signal to noise ratio", "Number of observations/Number of variables", "Polynomial degree")
rownames(result.table.svm) <- c("C","Epsilon","Error in estimating poly","Sparcity","CV Mean","Computational Time")
#save to a new file
Date<-gsub(pattern="-", replacement="_",Sys.Date())
fileName<-paste("SVM_results_table_KMLMM_term_project_",Date,".csv",sep="")
result.table.svm
setwd(dataDir)
write.table(result.table.svm, file=fileName, append=FALSE, row.names = TRUE, col.names = NA, sep = ";")


#define more relevant graphic i.e. most and best descriptive graphic
#pag. 28 <- reorder bars
#pag. 122 <- multiple histograms with different color
#pag. 159 <- error bars
#boxplots? ts?

#todo: is relevant to know the relation between str, cv.mean and sparcity?

#P.P. copy from tests done in isolated environment# values to consider: cv.mean, sparcity, sd.sparsity, 
#signal to noise ratio, comput time
#
#install.packages("ggplot2")
library(ggplot2)
#signal to noise ratio vs cv.mean
plot(d_low_model_quality$signal.to.noise.ratio, d_low_model_quality$cv.mean)
ggplot(d,aes(x=d$cv.mean,y=d$signal.to.noise.ratio,fill=d$id_parameter_combination))+geom_bar(position = "dodge",stat="identity")+labs(title="Comparison for CV Mean and STNR",x="CV Mean",y="Signal to noise ratio")
ggplot(d,aes(x=d$cv.mean,y=d$sparsity,fill=d$id_parameter_combination))+geom_bar(position = "identity",stat="identity")+labs(title="Comparison for CV Mean and STNR",x="CV Mean",y="Sparcity")
ggplot(d,aes(x=d$sparsity,y=d$signal.to.noise.ratio,fill=d$id_parameter_combination))+geom_bar(position = "identity",stat="identity")+labs(title="Comparison for CV Mean and STNR",x="Sparcity",y="Signal to noise ratio") 

#shows the same concentration, most of them tend to be more grouped in 1 (100 in the graph)
#ggplot(d_low_model_quality,aes(x=cv.mean,y=signal.to.noise.ratio,fill=id_parameter_combination))+geom_bar(position = "dodge",stat="identity")+labs(title="Comparison for CV Mean and STNR",x="CV Mean",y="Signal to noise ratio") 

#get the computation time for each parameter, in total 100 combinations,3 each
hist(tapply(d$compu.time,d$id_parameter_combination,sum))