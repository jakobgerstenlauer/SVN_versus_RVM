#remove old objects for safety resons
rm(list=ls(all=TRUE))

#utility function
glue<-function(...){paste(...,sep="")}

#TODO Adapt to working dir or remove!
#setwd("D:/Documents/MIRI/Semestre 2/ProyectoFinal/KernelFinalProject/code")
#setwd("J:/UPC/2016/02/KMLMM/KernelMethods/practicals/term_project/code")

if(!exists("codeDir")){
  if(basename(getwd())=='data')
    setwd('..')
  source("workingDir.R")
}

#define path of standard directories
source("workingDir.R")
setwd(dataDir)
d<-read.csv("Results_Simulation_SVM_KMLMM_term_project_2016_12_04.csv",sep=";")
str(d)
#there was a mistake in the first run: num.observations and num.vars were mixed up! why?
names(d)[3]<-"num.observations"
names(d)[4]<-"num.vars"

with(d,plot(signal.to.noise.ratio, cv.mean)) #the plot is already here?
#not very informative, nearly always 1!
with(d,hist(cv.mean))
#not very informative, nearly always 1!

#Why is the explained variance so freaking high?
#Which attributes explain low model quality?
#Too few variables?#I suppose this is the correction you tlaked me about

d_low_model_quality<-subset(d,cv.mean<0.9)

with(d, hist(num.vars))
with(d, hist(num.observations))
#Conclusion: the only data sets which were not perfectly well explained were those with the highest number of variables
#which were 13 and 11 and a relatively low number of observations!
#is this correct? I mean Tomas mentioned that a very good model is not probably correct, I know we know it is correct but
#this is just because we generate the data, ask

with(d_low_model_quality, table(print(num.observations/num.vars)) )
#5.23076923076923                8 
#10               10 

with(d, hist(num.observations/num.vars) )#todo: all this plots in a fancy way->r graphics cookbook
#a ratio of 5 and 8 is at the lower end of the distribution!!!
#Conclusion: We have to assure a ratio of observations/variables lower than 10 to get interesting results!

with(d, plot( num.observations/num.vars, cv.mean))

#Are there any other relevant attributes?
with(d, plot( signal.to.noise.ratio, cv.mean))
with(d, plot( polynomial.degree, cv.mean))
#The imperfect model only occurred with data generated by polynomial degree 3!#why?

#*****************************************************************************************
#Conclusion:
#In order to get more interesting results we have to adapt the latin hypercube:
#1) the ratio between num of observations to num of variable should be lower than 10
#2) at least 10 features
#3) polynomial degree three and higher
#4) signal-to-noise ratio higher than 0.3
#*****************************************************************************************
